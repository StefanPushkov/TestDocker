{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils import data as data_utils\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data \n",
    "df = pd.read_csv('/home/stefan/stef/Disk/Projects/SkillTask2/data/train.csv/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create copy of a dataframe\n",
    "data = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>f0</th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>f3</th>\n",
       "      <th>f4</th>\n",
       "      <th>f5</th>\n",
       "      <th>f6</th>\n",
       "      <th>f7</th>\n",
       "      <th>f8</th>\n",
       "      <th>...</th>\n",
       "      <th>f1603</th>\n",
       "      <th>f1604</th>\n",
       "      <th>f1605</th>\n",
       "      <th>f1606</th>\n",
       "      <th>f1607</th>\n",
       "      <th>f1608</th>\n",
       "      <th>f1609</th>\n",
       "      <th>f1610</th>\n",
       "      <th>f1611</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample_0</td>\n",
       "      <td>25.609375</td>\n",
       "      <td>6.703125</td>\n",
       "      <td>3.652344</td>\n",
       "      <td>10.039062</td>\n",
       "      <td>169.375</td>\n",
       "      <td>102.8125</td>\n",
       "      <td>1.422852</td>\n",
       "      <td>6.722656</td>\n",
       "      <td>8.015625</td>\n",
       "      <td>...</td>\n",
       "      <td>8.070312</td>\n",
       "      <td>4.363281</td>\n",
       "      <td>5.019531</td>\n",
       "      <td>5.710938</td>\n",
       "      <td>6.343750</td>\n",
       "      <td>6.843750</td>\n",
       "      <td>7.289062</td>\n",
       "      <td>7.617188</td>\n",
       "      <td>7.980469</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sample_1</td>\n",
       "      <td>18.343750</td>\n",
       "      <td>5.824219</td>\n",
       "      <td>2.966797</td>\n",
       "      <td>4.902344</td>\n",
       "      <td>164.625</td>\n",
       "      <td>71.8125</td>\n",
       "      <td>1.357422</td>\n",
       "      <td>5.894531</td>\n",
       "      <td>2.753906</td>\n",
       "      <td>...</td>\n",
       "      <td>7.359375</td>\n",
       "      <td>4.195312</td>\n",
       "      <td>4.808594</td>\n",
       "      <td>5.425781</td>\n",
       "      <td>5.949219</td>\n",
       "      <td>6.339844</td>\n",
       "      <td>6.730469</td>\n",
       "      <td>7.074219</td>\n",
       "      <td>7.175781</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sample_2</td>\n",
       "      <td>28.562500</td>\n",
       "      <td>6.230469</td>\n",
       "      <td>3.583984</td>\n",
       "      <td>7.882812</td>\n",
       "      <td>159.500</td>\n",
       "      <td>113.1875</td>\n",
       "      <td>1.696289</td>\n",
       "      <td>6.316406</td>\n",
       "      <td>4.605469</td>\n",
       "      <td>...</td>\n",
       "      <td>8.562500</td>\n",
       "      <td>4.523438</td>\n",
       "      <td>5.097656</td>\n",
       "      <td>5.789062</td>\n",
       "      <td>6.457031</td>\n",
       "      <td>6.871094</td>\n",
       "      <td>7.386719</td>\n",
       "      <td>7.878906</td>\n",
       "      <td>8.328125</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sample_3</td>\n",
       "      <td>28.062500</td>\n",
       "      <td>6.132812</td>\n",
       "      <td>2.726562</td>\n",
       "      <td>6.378906</td>\n",
       "      <td>169.750</td>\n",
       "      <td>111.0000</td>\n",
       "      <td>1.535156</td>\n",
       "      <td>6.199219</td>\n",
       "      <td>3.712891</td>\n",
       "      <td>...</td>\n",
       "      <td>4.558594</td>\n",
       "      <td>3.533203</td>\n",
       "      <td>3.900391</td>\n",
       "      <td>4.261719</td>\n",
       "      <td>4.042969</td>\n",
       "      <td>3.869141</td>\n",
       "      <td>3.890625</td>\n",
       "      <td>4.042969</td>\n",
       "      <td>4.273438</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sample_4</td>\n",
       "      <td>20.109375</td>\n",
       "      <td>6.144531</td>\n",
       "      <td>3.203125</td>\n",
       "      <td>6.035156</td>\n",
       "      <td>164.750</td>\n",
       "      <td>78.8750</td>\n",
       "      <td>1.281250</td>\n",
       "      <td>6.187500</td>\n",
       "      <td>4.003906</td>\n",
       "      <td>...</td>\n",
       "      <td>6.613281</td>\n",
       "      <td>4.625000</td>\n",
       "      <td>4.996094</td>\n",
       "      <td>5.328125</td>\n",
       "      <td>5.593750</td>\n",
       "      <td>5.800781</td>\n",
       "      <td>6.027344</td>\n",
       "      <td>6.242188</td>\n",
       "      <td>6.449219</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1614 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_id         f0        f1        f2         f3       f4        f5  \\\n",
       "0  sample_0  25.609375  6.703125  3.652344  10.039062  169.375  102.8125   \n",
       "1  sample_1  18.343750  5.824219  2.966797   4.902344  164.625   71.8125   \n",
       "2  sample_2  28.562500  6.230469  3.583984   7.882812  159.500  113.1875   \n",
       "3  sample_3  28.062500  6.132812  2.726562   6.378906  169.750  111.0000   \n",
       "4  sample_4  20.109375  6.144531  3.203125   6.035156  164.750   78.8750   \n",
       "\n",
       "         f6        f7        f8  ...     f1603     f1604     f1605     f1606  \\\n",
       "0  1.422852  6.722656  8.015625  ...  8.070312  4.363281  5.019531  5.710938   \n",
       "1  1.357422  5.894531  2.753906  ...  7.359375  4.195312  4.808594  5.425781   \n",
       "2  1.696289  6.316406  4.605469  ...  8.562500  4.523438  5.097656  5.789062   \n",
       "3  1.535156  6.199219  3.712891  ...  4.558594  3.533203  3.900391  4.261719   \n",
       "4  1.281250  6.187500  4.003906  ...  6.613281  4.625000  4.996094  5.328125   \n",
       "\n",
       "      f1607     f1608     f1609     f1610     f1611    y  \n",
       "0  6.343750  6.843750  7.289062  7.617188  7.980469  1.0  \n",
       "1  5.949219  6.339844  6.730469  7.074219  7.175781  1.0  \n",
       "2  6.457031  6.871094  7.386719  7.878906  8.328125  1.0  \n",
       "3  4.042969  3.869141  3.890625  4.042969  4.273438  1.0  \n",
       "4  5.593750  5.800781  6.027344  6.242188  6.449219  0.0  \n",
       "\n",
       "[5 rows x 1614 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace inf values with np.nan, then replace nan with 0\n",
    "data.replace([np.inf, -np.inf], np.nan,inplace=True)\n",
    "data = data.fillna(0) # Check mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features\n",
    "X = data.drop(['sample_id', 'y'], axis=1)\n",
    "# Labels\n",
    "y = data['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features normalization\n",
    "features_norm = StandardScaler() \n",
    "X_std = features_norm.fit_transform(X) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data in train/test\n",
    "X_train, x_test, Y_train, y_test = train_test_split(X_std, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To torch tensor: Train\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float)\n",
    "Y_train_tensor = torch.tensor(Y_train.values).flatten() \n",
    "\n",
    "# Test\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float)\n",
    "y_test_tensor = torch.tensor(y_test.values).flatten() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train dataset\n",
    "batch_size = 256\n",
    "\n",
    "train_dataset = data_utils.TensorDataset(X_train_tensor, Y_train_tensor) \n",
    "train_loader = data_utils.DataLoader(dataset = train_dataset, batch_size = batch_size, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256, 1612])\n",
      "torch.Size([256])\n"
     ]
    }
   ],
   "source": [
    "for data, labels in train_loader:\n",
    "    print(data.size())\n",
    "    print(labels.size())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([876, 1612])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128*6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "#our class must extend nn.Module\n",
    "class MyClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyClassifier,self).__init__()\n",
    "        #Our network consists of 3 layers. 1 input, 1 hidden and 1 output layer\n",
    "        #This applies Linear transformation to input data. \n",
    "        self.fc1 = nn.Linear(1612,3200)\n",
    "        self.fc2 = nn.Linear(3200,3200)\n",
    "        self.fc3 = nn.Linear(3200,3200)\n",
    "        self.fc4 = nn.Linear(3200,1600)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        #This applies linear transformation to produce output data\n",
    "        self.fc5 = nn.Linear(1600,1)\n",
    "        \n",
    "        self.bn0 = nn.BatchNorm1d(1612)\n",
    "        self.bn1 = nn.BatchNorm1d(3200)\n",
    "        self.bn2 = nn.BatchNorm1d(3200)\n",
    "        self.bn3 = nn.BatchNorm1d(3200)\n",
    "        self.bn4 = nn.BatchNorm1d(1600)\n",
    "        \n",
    "        \n",
    "        \n",
    "    #This must be implemented\n",
    "    def forward(self,x):\n",
    "        #Output of the first layer\n",
    "        x = self.bn0(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        x = self.bn1(x)\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        \n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout(x)     \n",
    "        x = F.relu(self.fc3(x))\n",
    "        \n",
    "        x = self.bn3(x)\n",
    "        x = self.dropout(x)  \n",
    "        x = F.relu(self.fc4(x))\n",
    "        \n",
    "        x = self.fc5(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    #This function takes an input and predicts the class, (0 or 1)        \n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x)\n",
    "            y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    \n",
    "        return torch.tensor(y_pred_tag, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "#our class must extend nn.Module\n",
    "class MyClassifierSecond(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyClassifierSecond,self).__init__()\n",
    "        #Our network consists of 3 layers. 1 input, 1 hidden and 1 output layer\n",
    "        #This applies Linear transformation to input data. \n",
    "        self.fc1 = nn.Linear(1612,800)\n",
    "        self.fc2 = nn.Linear(800,200)\n",
    "        self.layer_out = nn.Linear(200,1)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        #This applies linear transformation to produce output data\n",
    "        \n",
    "        \n",
    "        self.bn0 = nn.BatchNorm1d(1612)\n",
    "        self.bn1 = nn.BatchNorm1d(800)\n",
    "        self.bn2 = nn.BatchNorm1d(400)\n",
    "        self.bn_out = nn.BatchNorm1d(200)\n",
    "        \n",
    "        \n",
    "        \n",
    "    #This must be implemented\n",
    "    def forward(self,x):\n",
    "        #Output of the first layer\n",
    "        x = self.bn0(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x) \n",
    "        \n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        \n",
    "       \n",
    "        x = self.bn_out(x)         \n",
    "        x = self.layer_out(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "    #This function takes an input and predicts the class, (0 or 1)        \n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            y_pred = model(x)\n",
    "            y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
    "\n",
    "    \n",
    "        return torch.tensor(y_pred_tag, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize the model        \n",
    "model = MyClassifierSecond()\n",
    "#Define loss criterion\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "#Define the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MyClassifierSecond(\n",
       "  (fc1): Linear(in_features=1612, out_features=800, bias=True)\n",
       "  (fc2): Linear(in_features=800, out_features=200, bias=True)\n",
       "  (layer_out): Linear(in_features=200, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (bn0): BatchNorm1d(1612, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn1): BatchNorm1d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn2): BatchNorm1d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (bn_out): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optim, criterion, train_dl):\n",
    "    model.train()\n",
    "    total = 0\n",
    "    sum_loss = 0\n",
    "    for x, y in train_dl:\n",
    "        batch = y.shape[0]\n",
    "        output = model(x)\n",
    "        # output = F.softmax(output, dim=0)       \n",
    "        loss = criterion(output, y.unsqueeze(1))   \n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optim.step()\n",
    "        total += batch\n",
    "        sum_loss += batch*(loss.item())\n",
    "        # print(\"Batch loss: \", batch*(loss.item()))\n",
    "    return sum_loss/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss 0.9243452419685753\n",
      "Epoch 11, Loss 0.15776427462776935\n",
      "Epoch 21, Loss 0.07369973273727531\n",
      "Epoch 31, Loss 0.030707612186408915\n",
      "Epoch 41, Loss 0.021387442969100626\n",
      "Epoch 51, Loss 0.0259090316950162\n",
      "Epoch 61, Loss 0.01229165805017154\n",
      "Epoch 71, Loss 0.008375804739125692\n",
      "Epoch 81, Loss 0.005519446579092453\n",
      "Epoch 91, Loss 0.01406173242165165\n",
      "Epoch 101, Loss 0.009605087890576588\n",
      "Epoch 111, Loss 0.002867461197201396\n",
      "Epoch 121, Loss 0.006560417974389859\n",
      "Epoch 131, Loss 0.0030086146224558453\n",
      "Epoch 141, Loss 0.0021964803221500126\n",
      "Epoch 151, Loss 0.010981377876710796\n",
      "Epoch 161, Loss 0.0032133167282949442\n",
      "Epoch 171, Loss 0.008608913816045607\n",
      "Epoch 181, Loss 0.0017178328715152112\n",
      "Epoch 191, Loss 0.0007254079463913438\n",
      "Epoch 201, Loss 0.002269058323930099\n",
      "Epoch 211, Loss 0.006773565283375724\n",
      "Epoch 221, Loss 0.0020521640010875064\n",
      "Epoch 231, Loss 0.0016255472283514293\n",
      "Epoch 241, Loss 0.014469964455835418\n",
      "Epoch 251, Loss 0.006433949999280397\n",
      "Epoch 261, Loss 0.00612255999342936\n",
      "Epoch 271, Loss 0.0008121619829916789\n",
      "Epoch 281, Loss 0.0005953606257300935\n",
      "Epoch 291, Loss 0.001186346351400211\n",
      "Epoch 301, Loss 0.013006663831355582\n",
      "Epoch 311, Loss 0.0028852356158125614\n",
      "Epoch 321, Loss 0.0006824565790994368\n",
      "Epoch 331, Loss 0.002445960502051426\n",
      "Epoch 341, Loss 0.0002684954701746022\n",
      "Epoch 351, Loss 6.392597530739921e-05\n",
      "Epoch 361, Loss 0.0019811453273698354\n",
      "Epoch 371, Loss 0.0026756006713967895\n",
      "Epoch 381, Loss 2.461338571005858e-05\n",
      "Epoch 391, Loss 0.00013591845385792483\n",
      "Epoch 401, Loss 0.004674712045505423\n",
      "Epoch 411, Loss 0.0007059609699286264\n",
      "Epoch 421, Loss 0.0012387794742318556\n",
      "Epoch 431, Loss 0.004234569147365991\n",
      "Epoch 441, Loss 0.00043426622240987236\n",
      "Epoch 451, Loss 0.00011794492849219912\n",
      "Epoch 461, Loss 3.428457561737756e-05\n",
      "Epoch 471, Loss 0.0028594638713459026\n",
      "Epoch 481, Loss 0.0001191248577250778\n",
      "Epoch 491, Loss 5.911797771526208e-05\n"
     ]
    }
   ],
   "source": [
    "#Number of epochs\n",
    "epochs = 500\n",
    "#List to store losses\n",
    "losses = []\n",
    "for i in range(epochs):\n",
    "    epoch_loss = train_model(model=model, optim=optimizer, criterion=criterion, train_dl=train_loader)\n",
    "    losses.append(epoch_loss)\n",
    "    if i % 10 == 0:\n",
    "        print(\"Epoch {0}, Loss {1}\".format(i+1, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(range(epochs), losses)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('epoch');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.73      0.74        82\n",
      "           1       0.84      0.85      0.85       137\n",
      "\n",
      "    accuracy                           0.81       219\n",
      "   macro avg       0.80      0.79      0.79       219\n",
      "weighted avg       0.81      0.81      0.81       219\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-213-218269466e6b>:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(y_pred_tag, dtype=float)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "y_preds = model.predict(x_test_tensor)\n",
    "\n",
    "print(metrics.classification_report(y_test_tensor.long(), y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-213-218269466e6b>:46: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(y_pred_tag, dtype=float)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.790413031867545"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(y_test_tensor.long(), model.predict(x_test_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
